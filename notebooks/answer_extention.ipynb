{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import stanza\n",
    "import logging\n",
    "from underthesea import word_tokenize\n",
    "from utils import (\n",
    "    INPUT_FILE,\n",
    "    LOG_FORMAT,\n",
    "    DATE_FORMAT,\n",
    "    ENTITY_TYPES,\n",
    "    SPAN_TYPES,\n",
    "    DATA_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT, datefmt=DATE_FORMAT)\n",
    "\n",
    "PARSER = stanza.Pipeline(\n",
    "    lang=\"vi\",\n",
    "    processors=\"tokenize, pos, ner, constituency\",\n",
    "    use_gpu=True,\n",
    "    device=0,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/2023 15:21:42 - INFO - Loading span NE...\n",
      "24/10/2023 15:21:42 - INFO - Loading span NP...\n",
      "24/10/2023 15:21:42 - INFO - Loading span AP...\n",
      "24/10/2023 15:21:42 - INFO - Loading span VP...\n",
      "24/10/2023 15:21:42 - INFO - Loading span S...\n",
      "24/10/2023 15:21:42 - INFO - Loading TVPL...\n"
     ]
    }
   ],
   "source": [
    "raws = {}\n",
    "extracted_answers = {}\n",
    "\n",
    "for span_type in SPAN_TYPES:\n",
    "    logging.info(\"Loading span {}...\".format(span_type))\n",
    "    with open(\n",
    "        os.path.join(\n",
    "            DATA_DIR,\n",
    "            f\"{INPUT_FILE}_answer_extract_{span_type}.json\",\n",
    "        ),\n",
    "        \"r\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as file:\n",
    "        raws[span_type] = json.load(file)\n",
    "    extracted_answers[span_type] = {}\n",
    "\n",
    "logging.info(\"Loading TVPL...\")\n",
    "with open(\n",
    "    os.path.join(\n",
    "        DATA_DIR,\n",
    "        f\"{INPUT_FILE}_1k_ViSummary.json\",\n",
    "    ),\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as file:\n",
    "    tvpl = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/2023 15:21:42 - INFO - uid2tvpl exists! Logging...\n"
     ]
    }
   ],
   "source": [
    "tvpl_data = {}\n",
    "if not os.path.exists(os.path.join(DATA_DIR, f\"uid2{INPUT_FILE}.json\")):\n",
    "    logging.info(\"Process tvpl...\")\n",
    "    for item in tqdm(tvpl, desc=\"process tvpl\"):\n",
    "        uid = item[\"url\"]\n",
    "        for sum_idx, sum_item in enumerate(item[\"summary\"]):\n",
    "            sum_item = \" \".join(word_tokenize(sum_item))\n",
    "            tokens = PARSER(sum_item).sentences[0].constituency.leaf_labels()\n",
    "            sum_item = \" \".join(tokens)\n",
    "            item[\"summary\"][sum_idx] = sum_item\n",
    "        tvpl_data[uid] = {\n",
    "            \"summary\": item[\"summary\"],\n",
    "            \"document\": item[\"document\"],\n",
    "        }\n",
    "    with open(\n",
    "        os.path.join(DATA_DIR, f\"uid2{INPUT_FILE}.json\"), \"w\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        json.dump(\n",
    "            tvpl_data,\n",
    "            f,\n",
    "            indent=4,\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "else:\n",
    "    logging.info(f\"uid2{INPUT_FILE} exists! Logging...\")\n",
    "    with open(\n",
    "        os.path.join(DATA_DIR, f\"uid2{INPUT_FILE}.json\"), \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        tvpl_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NE preprocess...: 100%|██████████| 1000/1000 [00:00<00:00, 372595.19it/s]\n",
      "NP preprocess...: 100%|██████████| 1000/1000 [00:00<00:00, 54708.79it/s]\n",
      "AP preprocess...: 100%|██████████| 1000/1000 [00:00<00:00, 498728.18it/s]\n",
      "VP preprocess...: 100%|██████████| 1000/1000 [00:00<00:00, 99457.08it/s]\n",
      "S preprocess...: 100%|██████████| 1000/1000 [00:00<00:00, 496367.34it/s]\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "\n",
    "for span_type in SPAN_TYPES:\n",
    "    for example in tqdm(raws[span_type], desc=\"{} preprocess...\".format(span_type)):\n",
    "        for p in example[\"entry\"]:\n",
    "            context = p[\"context\"]\n",
    "            for qa in p[\"QA\"]:\n",
    "                question = qa[\"question\"]\n",
    "                question = (\n",
    "                    question.replace(\"(\", \"-LRB-\")\n",
    "                    .replace(\")\", \"-RRB-\")\n",
    "                    .replace(\"[\", \"-LSB-\")\n",
    "                )\n",
    "                question = (\n",
    "                    question.replace(\"]\", \"-RSB-\")\n",
    "                    .replace(\"{\", \"-LCB-\")\n",
    "                    .replace(\"}\", \"-RCB-\")\n",
    "                )\n",
    "\n",
    "                uid = qa[\"id\"].split(\"_\")[0]\n",
    "                question_first_part = None\n",
    "                question_second_part = None\n",
    "                for entity_type in ENTITY_TYPES:\n",
    "                    if len(question.split(entity_type)) > 1:\n",
    "                        question_first_part = question.split(entity_type)[0]\n",
    "                        question_second_part = question.split(entity_type)[1]\n",
    "                        break\n",
    "                if question_first_part is not None or question_second_part is not None:\n",
    "                    error += 1\n",
    "                    continue\n",
    "\n",
    "                # assert (\n",
    "                #     question_first_part is not None or question_second_part is not None\n",
    "                # ), f\"q: {question}, first: {question_first_part}, second: {question_second_part}\"\n",
    "\n",
    "                summary = tvpl_data[uid][\"summary\"]\n",
    "                summary = [s.replace(\"(\", \"-LRB-\") for s in summary]\n",
    "                summary = [s.replace(\")\", \"-RRB-\") for s in summary]\n",
    "                summary = [s.replace(\"[\", \"-LSB-\") for s in summary]\n",
    "                summary = [s.replace(\"]\", \"-RSB-\") for s in summary]\n",
    "                summary = [s.replace(\"{\", \"-LCB-\") for s in summary]\n",
    "                summary = [s.replace(\"}\", \"-RCB-\") for s in summary]\n",
    "                summary_idx = -1\n",
    "\n",
    "                for sum_idx, summary_item in enumerate(summary):\n",
    "                    print(\"...................\")\n",
    "                    if (\n",
    "                        len(summary_item) >= 1\n",
    "                        and summary_item.find(question_first_part) != -1\n",
    "                    ):\n",
    "                        summary_idx = sum_idx\n",
    "                        break\n",
    "                    if (\n",
    "                        len(question_second_part) >= 1\n",
    "                        and summary_item.find(question_second_part) != -1\n",
    "                    ):\n",
    "                        summary_idx = sum_idx\n",
    "                        break\n",
    "                if (\n",
    "                    summary_idx == -1\n",
    "                    and len(question_first_part) == 0\n",
    "                    and len(question_second_part) == 0\n",
    "                ):\n",
    "                    pass\n",
    "                else:\n",
    "                    error += 1\n",
    "                    continue\n",
    "                    assert (\n",
    "                        summary_idx != -1\n",
    "                    ), f\"qa: {qa}, summary: {summary}, first: ***{question_first_part}***,***{question_second_part}***; second: ****{summary[0].find(question_first_part)}****,****{summary[0].find(question_second_part)}****\"\n",
    "\n",
    "                qa[\"summary_idx\"] = summary_idx\n",
    "                answer = qa[\"answers\"][0]\n",
    "                answer_start = answer[\"answer_start\"]\n",
    "                answer_text = answer[\"text\"]\n",
    "                answer_end = answer_start + len(\n",
    "                    answer_text\n",
    "                )  # [answer_start, answer_end)\n",
    "                if uid not in extracted_answers[span_type]:\n",
    "                    extracted_answers[span_type][uid] = {}\n",
    "                if summary_idx not in extracted_answers[span_type][uid]:\n",
    "                    extracted_answers[span_type][uid][summary_idx] = []\n",
    "                assert answer_text == context[answer_start:answer_end]\n",
    "                extracted_answers[span_type][uid][summary_idx].append(\n",
    "                    {\n",
    "                        \"question\": question,\n",
    "                        \"text\": answer_text,\n",
    "                        \"answer_start\": answer_start,\n",
    "                        \"answer_end\": answer_end,\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in tqdm(raws[\"NE\"][\"data\"], desc=\"extend NE answers...\"):\n",
    "    for p in example[\"paragraphs\"]:\n",
    "        context = p[\"context\"]\n",
    "        for qa in p[\"qas\"]:\n",
    "            qid = qa[\"id\"]\n",
    "            uid = qid.split(\"_\")[0]\n",
    "            summary_idx = qa[\"summary_idx\"]\n",
    "            qa[\"answers\"][0][\"span_type\"] = \"NE\"\n",
    "            for entity_type in ENTITY_TYPES:\n",
    "                if entity_type != \"PLACEHOLDER\" and entity_type in qa[\"question\"]:\n",
    "                    qa[\"answers\"][0][\"entity_type\"] = entity_type\n",
    "            if summary_idx == -1:\n",
    "                continue\n",
    "            answer_start = qa[\"answers\"][0][\"answer_start\"]\n",
    "            answer_end = answer_start + len(qa[\"answers\"][0][\"text\"])\n",
    "            for span_type in SPAN_TYPES:\n",
    "                if span_type == \"NE\":\n",
    "                    continue\n",
    "                if uid not in extracted_answers[span_type]:\n",
    "                    continue\n",
    "                if summary_idx not in extracted_answers[span_type][uid]:\n",
    "                    continue\n",
    "                answer_ranges = extracted_answers[span_type][uid][summary_idx]\n",
    "                for answer_range in answer_ranges:\n",
    "                    if (\n",
    "                        answer_range[\"answer_start\"] < answer_start\n",
    "                        and answer_range[\"answer_end\"] >= answer_end\n",
    "                        or answer_range[\"answer_start\"] <= answer_start\n",
    "                        and answer_range[\"answer_end\"] > answer_end\n",
    "                    ):\n",
    "                        clause = qa[\"question\"] + \" \" + qa[\"answers\"][0][\"text\"]\n",
    "                        clause_len = len(clause.split()) - 1  # ignore the placeholder\n",
    "                        candidate_ans_len = len(answer_range[\"text\"].split())\n",
    "                        if clause_len * 0.8 < candidate_ans_len:\n",
    "                            continue\n",
    "                        qa[\"answers\"][0][\"span_type\"] = span_type\n",
    "                        qa[\"answers\"][0][\"answer_start\"] = answer_range[\"answer_start\"]\n",
    "                        qa[\"answers\"][0][\"text\"] = answer_range[\"text\"]\n",
    "                        qa[\"question\"] = answer_range[\"question\"].replace(\n",
    "                            \"PLACEHOLDER\", qa[\"answers\"][0][\"entity_type\"]\n",
    "                        )\n",
    "                        answer_start = answer_range[\"answer_start\"]\n",
    "                        answer_end = answer_range[\"answer_end\"]\n",
    "                        assert answer_range[\"text\"] == context[answer_start:answer_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(DATA_DIR, \"cloze_clause_tvpl _data_diverse_answer_span_80.json\"),\n",
    "    \"w\",\n",
    "    encoding=\"utf-8\",\n",
    ") as f:\n",
    "    json.dump(raws[\"NE\"], f, indent=4)\n",
    "\n",
    "raw = raws[\"NE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span2cnt = {}\n",
    "for example in tqdm(raw[\"data\"], desc=\"span stat\"):\n",
    "    for p in example[\"paragraphs\"]:\n",
    "        for qa in p[\"qas\"]:\n",
    "            ans_type = qa[\"answers\"][0][\"span_type\"]\n",
    "            if ans_type not in span2cnt:\n",
    "                span2cnt[ans_type] = 1\n",
    "            else:\n",
    "                span2cnt[ans_type] += 1\n",
    "logging.info(span2cnt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VietLegalQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
